{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734d2f4b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using 1D Convolutional Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a485247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learnremote.medium.com/sentiment-analysis-using-1d-convolutional-neural-networks-part-1-f8b6316489a2\n",
    "# https://github.com/gizenmtl/IMDB-Sentiment-Analysis-and-Text-Classification/tree/master/aclImdb/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af00f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "imdb_dir = 'D:/SentimentAnalysis/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = list()\n",
    "texts = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1430db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the labels of the raw IMDB data\n",
    "for label_type in ['neg', 'pos', 'unsup']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            if label_type == 'pos':\n",
    "                labels.append(1)                \n",
    "            else:\n",
    "                labels.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90b6b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 693 unique tokens. \n",
      "Data Shape: (9, 500)\n",
      "Shape of data tensor:  (9, 500)\n",
      "Shape of label tensor:  (12,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# cut off reviews after 500 words\n",
    "max_len = 500 \n",
    "# train on 10000 samples\n",
    "training_samples = 10000\n",
    " # validate on 10000 samples \n",
    "validation_samples = 10000\n",
    "# consider only the top 10000 words\n",
    "max_words = 10000 \n",
    "\n",
    "tokenizer_path = 'tokenizer'\n",
    "# import tokenizer with the consideration for only the top 500 words\n",
    "tokenizer = Tokenizer(num_words=max_words) \n",
    "# fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(texts) \n",
    "# convert the texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts) \n",
    "\n",
    "# save the tokenizer\n",
    "with open(os.path.join(tokenizer_path, 'tokenizer_m1.pickle'), 'wb') as handle:\n",
    "    pk.dump(tokenizer, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens. ' % len(word_index))\n",
    "\n",
    " # pad the sequence to the required length to ensure uniformity\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "print('Data Shape: {}'.format(data.shape))\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print(\"Shape of data tensor: \", data.shape)\n",
    "print(\"Shape of label tensor: \", labels.shape)\n",
    "\n",
    "# split the data into training and validation set but before that shuffle it first\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:training_samples + validation_samples]\n",
    "y_val = labels[training_samples:training_samples + validation_samples]\n",
    "\n",
    "# test_data\n",
    "x_test = data[training_samples+validation_samples:]\n",
    "y_test = labels[training_samples+validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9e9fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 498, 256)          38656     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 166, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 164, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 54, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 52, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 17, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,392,161\n",
      "Trainable params: 1,392,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras import Model, layers\n",
    "from keras import Input\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        patience=1,\n",
    "        monitor='acc',\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir_m1',\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1,\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        filepath='model/movie_sentiment_m1.h5',\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        patience=1,\n",
    "        factor=0.1,\n",
    "    )\n",
    "]\n",
    "\n",
    "# layer developing\n",
    "text_input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(max_words, 50)(text_input_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(embedding_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = GlobalMaxPooling1D()(text_layer)\n",
    "text_layer = Dense(256, activation='relu')(text_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(text_layer)\n",
    "model = Model(text_input_layer, output_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b4fded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: -0.1374 - acc: 0.3333\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: -1.2078 - acc: 0.3333\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: -8.4243 - acc: 0.3333\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: -42.6562 - acc: 0.3333\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: -165.8541 - acc: 0.3333\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: -536.3954 - acc: 0.3333\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: -1493.8833 - acc: 0.3333\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: -3681.5793 - acc: 0.3333\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: -8146.4150 - acc: 0.3333\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: -16578.8027 - acc: 0.3333\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: -31409.2852 - acc: 0.3333\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: -56217.4180 - acc: 0.3333\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: -95753.4844 - acc: 0.3333\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: -156502.3750 - acc: 0.3333\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: -246406.8594 - acc: 0.3333\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: -376845.1250 - acc: 0.3333\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: -560071.6250 - acc: 0.3333\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: -812158.4375 - acc: 0.3333\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: -1152542.7500 - acc: 0.3333\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: -1602957.5000 - acc: 0.3333\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: -2190207.2500 - acc: 0.3333\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: -2943831.0000 - acc: 0.3333\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: -3898697.2500 - acc: 0.3333\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: -5094577.0000 - acc: 0.3333\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: -6573946.0000 - acc: 0.3333\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: -8392142.0000 - acc: 0.3333\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: -10601328.0000 - acc: 0.3333\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: -13262093.0000 - acc: 0.3333\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: -16451972.0000 - acc: 0.3333\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: -20239354.0000 - acc: 0.3333\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: -24706800.0000 - acc: 0.3333\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: -29962824.0000 - acc: 0.3333\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: -36100936.0000 - acc: 0.3333\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: -43224912.0000 - acc: 0.3333\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 181ms/step - loss: -51448604.0000 - acc: 0.3333\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: -60933140.0000 - acc: 0.3333\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: -71799720.0000 - acc: 0.3333\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: -84190848.0000 - acc: 0.3333\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: -98273664.0000 - acc: 0.3333\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: -114250584.0000 - acc: 0.3333\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: -132347960.0000 - acc: 0.3333\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: -152676544.0000 - acc: 0.3333\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: -175464064.0000 - acc: 0.3333\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: -201027792.0000 - acc: 0.3333\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: -229533952.0000 - acc: 0.3333\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: -261078864.0000 - acc: 0.3333\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: -296501184.0000 - acc: 0.3333\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: -335588608.0000 - acc: 0.3333\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: -378722176.0000 - acc: 0.3333\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: -426429024.0000 - acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# multi-input test\n",
    "#history = model.fit(x_train, y_train, epochs=50, batch_size=128, callbacks=callback_list, validation_data=(x_val, y_val))\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef0c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01838068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_rating(score, decoded_review):\n",
    "    if float(score) >= 0.9:\n",
    "        print('Review: {}\\nSentiment: Strongly Positive\\nScore: {}'.format(decoded_review, score))\n",
    "    elif float(score) >= 0.7 and float(score) < 0.9:\n",
    "        print('Review: {}\\nSentiment: Positive\\nScore: {}'.format(decoded_review, score))\n",
    "    elif float(score) >= 0.5 and float(score) < 0.7:\n",
    "        print('Review: {}\\nSentiment: Okay\\nScore: {}'.format(decoded_review, score))\n",
    "    else:\n",
    "        print('Review: {}\\nSentiment: Negative\\nScore: {}'.format(decoded_review, score))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea2ad551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text_list):\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(text_list)\n",
    "    data = pad_sequences(sequences, maxlen=500)\n",
    "\n",
    "    # decode the words\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    decoded_review = ' '.join([reverse_word_index.get(i, '?') for i in sequences[0]])\n",
    "    return decoded_review, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "522c3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_review(source=None, file_type=None):\n",
    "    '''\n",
    "    source: the text, as either a string or a list of strings\n",
    "    file_type: (str): indicating whether we expecting a file containing the\n",
    "    text data or a directory containing a list files holding the text\n",
    "    options: 'file' or 'dir'\n",
    "    '''\n",
    "    text_list = list()\n",
    "    if isinstance(source, str) and file_type is None:\n",
    "        text_list.append(source)\n",
    "        decoded_review, data = decode_review(text_list)\n",
    "        # make prediction\n",
    "        score = model.predict(data)[0][0]\n",
    "        review_rating(score, decoded_review)\n",
    "    \n",
    "    if isinstance(source, list) and file_type is None:\n",
    "        for item in source:\n",
    "            text_list = list()\n",
    "            text_list.append(item)\n",
    "            decoded_review, data = decode_review(text_list)\n",
    "            score = model.predict(data)[0][0]\n",
    "            review_rating(score, decoded_review)\n",
    "    \n",
    "    if isinstance(source, str) and file_type == 'file':\n",
    "        file_data = open(source).read()\n",
    "        text_list.append(file_data)\n",
    "        decoded_review, data = decode_review(text_list)\n",
    "        # make prediction\n",
    "        score = model.predict(data)[0][0]\n",
    "        review_rating(score, decoded_review)\n",
    "    \n",
    "    if isinstance(source, str) and file_type == 'dir':\n",
    "        file_content_holder = list()\n",
    "        for fname in os.listdir(source):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(source, fname))\n",
    "                file_content_holder.append(f.read())\n",
    "                f.close()\n",
    "        for item in file_content_holder:\n",
    "            text_list = list()\n",
    "            text_list.append(item)\n",
    "            decoded_review, data = decode_review(text_list)\n",
    "            score = model.predict(data)[0][0]\n",
    "            review_rating(score, decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e86f5b7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1757: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-2db52ff095d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore_review\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/SentimentAnalysis/aclImdb/test/neg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dir'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-af8114021949>\u001b[0m in \u001b[0;36mscore_review\u001b[1;34m(source, file_type)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mfile_content_holder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_content_holder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ramaswamy\\.conda\\envs\\sentibert\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1757: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "score_review('D:/SentimentAnalysis/aclImdb/test/neg', file_type='dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a05437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1a232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284bd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c10861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
